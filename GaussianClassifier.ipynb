{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c1cfca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b66765",
   "metadata": {},
   "source": [
    "1. Extend the Gaussian Naive Bayes code so that it handles missing values. Gaussian Naive Bayes can handle missing values in training by calculating conditional probabilities on the values that are present. You may choose to put a limit on the\n",
    "number of missing values allowed. Your code should also handle missing values on any test data. The easiest way to do\n",
    "this is to leave features with missing values out of the posterior probability calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7284dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGaussianNB(BaseEstimator, ClassifierMixin):   \n",
    "    def fit(self, Xt, yt):\n",
    "        self.var_smoothing = 1e-9   # zero variance will cause division by zero errors.\n",
    "        self.Xt = Xt\n",
    "        self.yt = yt\n",
    "        self.n_feat = Xt.shape[1]\n",
    "        self.mus = {}\n",
    "        self.sig_sqs = {}\n",
    "        self.priors = {}\n",
    "        c_dict = Counter(self.yt)\n",
    "        \n",
    "        for c in c_dict.keys():\n",
    "            self.mus[c] = np.zeros(self.n_feat) # where the means will be stored\n",
    "            self.sig_sqs[c] = np.zeros(self.n_feat) # where the variances will be stored\n",
    "            self.priors[c] = c_dict[c]/Xt.shape[0]\n",
    "            \n",
    "            mask = self.yt == c\n",
    "            X_tr_c = self.Xt[mask, :] # the rows for this class label\n",
    "            \n",
    "            for f in range(self.n_feat):\n",
    "                self.mus[c][f] = np.nanmean(X_tr_c[:,f])  # Changing the mean to nanmean to leave the nans out of conditional probability\n",
    "                self.sig_sqs[c][f] = np.nanvar(X_tr_c[:,f] + self.var_smoothing)  # Similarly Changing the variance to nanvariance to leave the nans out of conditional probability      \n",
    "        #print(self.mus)\n",
    "        #print(self.sig_sqs)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    # The predictions are the most common class in the training set.\n",
    "    def predict(self, Xtes):\n",
    "        #print(\"Predicting MGNB\")\n",
    "        self.Xtes = Xtes\n",
    "         \n",
    "        res_list = []\n",
    "        for sample in Xtes:\n",
    "            res_list.append(self.predict_single(sample))\n",
    "            \n",
    "        return np.array(res_list)\n",
    "    \n",
    "    def predict_single(self, x_single):\n",
    "        probs = {}\n",
    "        for c in self.priors.keys():   # for each of the class labels\n",
    "            probs[c] = self.priors[c]\n",
    "            \n",
    "            for i, f in enumerate(x_single):\n",
    "                if np.isnan(f):\n",
    "                    pxi_y = 1\n",
    "                else:\n",
    "                    t1 = 1/math.sqrt(2*math.pi*self.sig_sqs[c][i])\n",
    "                    num = (f - self.mus[c][i])**2\n",
    "                    den = 2*self.sig_sqs[c][i]\n",
    "                    pxi_y = t1 * math.exp(-num/den)\n",
    "                probs[c] = probs[c] * pxi_y\n",
    "                #print(t1, num, den, pxi_y)\n",
    "                #print(probs)\n",
    "            #print(c, self.priors[c])\n",
    "        return max(probs, key=probs.get) # Return the key with the largest value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fidelity_tests (X,y, nreps = 10):\n",
    "    for rs in range(1, nreps + 1):\n",
    "        X_tr_raw, X_ts_raw, y_train, y_test = train_test_split(X, y, \n",
    "                                                               random_state=rs, \n",
    "                                                               test_size=1/2)\n",
    "        # Scaling using Standard scaler\n",
    "        scale = StandardScaler()\n",
    "        X_train = scale.fit_transform(X_tr_raw)\n",
    "        X_test = scale.transform(X_ts_raw)\n",
    "        \n",
    "        # Univariate Imputing using SimpleImputer\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        imp.fit(X_train)\n",
    "        X_train_si = imp.transform(X_train)\n",
    "        X_test_si = imp.transform(X_test)\n",
    "        \n",
    "        # Multivariate Imputing using IterativeImputer\n",
    "        imp_it = IterativeImputer(missing_values=np.nan, max_iter = 11, random_state=0)\n",
    "        imp_it.fit(X_train)\n",
    "        X_train_it = imp_it.transform(X_train)\n",
    "        X_test_it = imp_it.transform(X_test)\n",
    "        \n",
    "        gnb_Simple = GaussianNB()\n",
    "        mgnb = MyGaussianNB()\n",
    "        gnb_Iterative = GaussianNB()\n",
    "        \n",
    "        gnb_Simple.fit(X_train_si,y_train)\n",
    "        mgnb.fit(X_train,y_train)\n",
    "        gnb_Iterative.fit(X_train_it,y_train)\n",
    "        \n",
    "        # Check accuracies against models.\n",
    "        ascore = accuracy_score(gnb_Simple.predict(X_test_si),mgnb.predict(X_test)) \n",
    "        gnb_acc_simple = accuracy_score(gnb_Simple.predict(X_test_si),y_test)\n",
    "        mgnb_acc = accuracy_score(mgnb.predict(X_test),y_test)\n",
    "        \n",
    "        print (\"Run after Univariate Imputing: %d Score: %.2f SK acc: %.2f My acc: %.2f\" % (rs, ascore, gnb_acc_simple, mgnb_acc))\n",
    "        \n",
    "        # Check accuracies against models.\n",
    "        ascore = accuracy_score(gnb_Iterative.predict(X_test_it),mgnb.predict(X_test))\n",
    "        gnb_acc_it = accuracy_score(gnb_Iterative.predict(X_test_it),y_test)\n",
    "        mgnb_acc = accuracy_score(mgnb.predict(X_test),y_test)\n",
    "        \n",
    "        print (\"Run after Multivariate Imputing: %d Score: %.2f SK acc: %.2f My acc: %.2f\" % (rs, ascore, gnb_acc_it, mgnb_acc))\n",
    "        \n",
    "        My_scores = cross_val_score(mgnb, X_train, y_train, cv=10)\n",
    "        print(\"CV scores for MyGNB: %.2f\" %(My_scores.mean()))\n",
    "        \n",
    "        Simple_gnb_scores = cross_val_score(gnb_Simple, X_train_si, y_train, cv=10)\n",
    "        print(\"CV scores for Simple imputed GNB SK: %.2f\" %(Simple_gnb_scores.mean()))\n",
    "        \n",
    "        Iterative_gnb_scores = cross_val_score(gnb_Iterative, X_train_it, y_train, cv=10)\n",
    "        print(\"CV scores for Iterative imputed GNB SK:%.2f\" %(Iterative_gnb_scores.mean()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2b38b",
   "metadata": {},
   "source": [
    "2. Test the performance of your implementation against the scikit-learn `GaussianNB` using missing value imputation. Test two imputation options, one univariate and one multi-variate. To help with your evaluation two versions of the penguins datasets with missing values are provided, one with 20% missing and the other with 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168285a8",
   "metadata": {},
   "source": [
    "## Penguins Dataset (20% Missing Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bbd8a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "penguins_20 = pd.read_csv('PenguinsMV0.2.csv', index_col = 0)\n",
    "penguins_20 = penguins_20.replace('?',np.nan)\n",
    "print(penguins_20.shape)\n",
    "penguins_20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = penguins_20.pop('species').values\n",
    "X_raw = penguins_20.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b19bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fidelity_tests(X_raw, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d259f",
   "metadata": {},
   "source": [
    "## Findings\n",
    "Because the missing values probability is ignored in my case, but is imputed to the column's mean value in GaussianNB, the above dataset is less accurate than MyGaussianNB after being simple imputed and iteratively imputed. Cross validation, on the other hand, provides almost equal results for imputed  but is `less accurate` than `MyGaussianNB` with CV accuracy of 94% for MyGaussianNB and 93% for imputed GausssianNB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd869b42",
   "metadata": {},
   "source": [
    "## Penguins Dataset (40% Missing Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebded81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_40 = pd.read_csv('PenguinsMV0.4.csv', index_col = 0)\n",
    "penguins_40 = penguins_40.replace('?',np.nan)\n",
    "print(penguins_40.shape)\n",
    "penguins_40.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d248f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_40 = penguins_40.pop('species').values\n",
    "X_raw_40 = penguins_40.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eeac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fidelity_tests(X_raw_40, y_40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4aa8b",
   "metadata": {},
   "source": [
    "## Findings:\n",
    "The above dataset after being simple imputed and Iteratively imputed is less accurate than MyGaussianNB because the missing values probability is ignored in my case but it is imputed to the mean value of the column in GaussianNB. However, cross validation yields results that are more accurate for MyGNB than the imputed  GaussianNB classifier with 87% for MyGaussianNB, 83% and 84% for univariate and multivariate imputations respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45d16f",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "It is now evident after testing the performance of `MyGaussianNB` Classifier against `GaussianNB` implementation in scikit-learn that `MyGaussianNB` is producing better accuracy than both the imputing methods on both Penguin datasets. This outcome is due to the way the imputers have strategised to replacing missing value by the mean values whereas in the case of `MyGaussianNB` the missing values are dumped in the computation of the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad90dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
